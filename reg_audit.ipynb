{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "SISTEMA DE AUDITORÍA Y TRAZABILIDAD - PIPELINE BIG DATA\n",
    "Universidad Politécnica Metropolitana de Hidalgo\n",
    "Maestría en Inteligencia Artificial - Big Data\n",
    "\n",
    "Autores: Sánchez Ríos José Luis, Santos Martínez Víctor Manuel\n",
    "Fecha: 17 de enero de 2026\n",
    "Profesor: Dr. Jaime Aguilar Ortiz\n",
    "\n",
    "Este módulo implementa un sistema robusto de auditoría con trazabilidad completa\n",
    "para garantizar integridad, repetibilidad y conformidad con estándares de data engineering.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from typing import Dict, List, Optional, Any\n",
    "from enum import Enum\n",
    "from io import StringIO\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Configuración de entrada: Logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('audit_pipeline.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Enumeraciones para tipos y constantes comunes\n",
    "\n",
    "class SourceType(Enum):\n",
    "    \"\"\"Tipos de fuentes de datos soportadas\"\"\"\n",
    "    TXT = \"txt\"\n",
    "    CSV = \"csv\"\n",
    "    HTML = \"html\"\n",
    "    JSON = \"json\"\n",
    "    XML = \"xml\"\n",
    "\n",
    "\n",
    "class AuditStatus(Enum):\n",
    "    \"\"\"Estados de auditoría\"\"\"\n",
    "    SUCCESS = \"success\"\n",
    "    FAILURE = \"failure\"\n",
    "    WARNING = \"warning\"\n",
    "    PENDING = \"pending\"\n",
    "\n",
    "\n",
    "# Configuración de URLs\n",
    "DATA_SOURCES = {\n",
    "    \"TXT\": \"https://raw.githubusercontent.com/VManuelSM/Actividad_1_Big_Data/refs/heads/main/logs_sistema.txt\",\n",
    "    \"CSV\": \"https://github.com/VManuelSM/Actividad_1_Big_Data/raw/refs/heads/main/registros_eventos.csv\",\n",
    "    \"HTML\": \"https://github.com/VManuelSM/Actividad_1_Big_Data/raw/refs/heads/main/reporte_incidencias.html\"\n",
    "}\n",
    "\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "AUDIT_DIR = \"audit_logs\"\n",
    "\n",
    "# Crear directorios\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(AUDIT_DIR, exist_ok=True)\n",
    "\n",
    "# Clases de datos para metadatos y registros de auditoría:\n",
    "\n",
    "@dataclass\n",
    "class HashMetadata:\n",
    "    \"\"\"Metadatos de integridad criptográfica\"\"\"\n",
    "    sha256: str\n",
    "    md5: str\n",
    "    timestamp: str\n",
    "    algorithm_version: str = \"SHA-256, MD5\"\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, str]:\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataQualityMetrics:\n",
    "    \"\"\"Métricas de calidad de datos\"\"\"\n",
    "    total_records: int\n",
    "    null_count: int\n",
    "    duplicate_count: int\n",
    "    data_types: Dict[str, str]\n",
    "    completeness_ratio: float\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SourceMetadata:\n",
    "    \"\"\"Metadatos completos de la fuente de datos\"\"\"\n",
    "    source_id: str\n",
    "    source_name: str\n",
    "    source_type: str\n",
    "    source_url: str\n",
    "    size_bytes: int\n",
    "    encoding: str\n",
    "    processing_timestamp: str\n",
    "    hash_metadata: HashMetadata\n",
    "    quality_metrics: Optional[DataQualityMetrics] = None\n",
    "    additional_info: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        data = asdict(self)\n",
    "        data['hash_metadata'] = self.hash_metadata.to_dict()\n",
    "        if self.quality_metrics:\n",
    "            data['quality_metrics'] = self.quality_metrics.to_dict()\n",
    "        return data\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AuditRecord:\n",
    "    \"\"\"Registro completo de auditoría\"\"\"\n",
    "    audit_id: str\n",
    "    pipeline_version: str\n",
    "    execution_timestamp: str\n",
    "    sources_processed: List[SourceMetadata]\n",
    "    audit_status: str\n",
    "    total_sources: int\n",
    "    successful_sources: int\n",
    "    failed_sources: int\n",
    "    warnings: List[str] = field(default_factory=list)\n",
    "    errors: List[str] = field(default_factory=list)\n",
    "    environment_info: Dict[str, str] = field(default_factory=dict)\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        data = asdict(self)\n",
    "        data['sources_processed'] = [s.to_dict() for s in self.sources_processed]\n",
    "        return data\n",
    "    \n",
    "    def save_to_json(self, filepath: str) -> None:\n",
    "        \"\"\"Guarda el registro de auditoría en formato JSON\"\"\"\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.to_dict(), f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Auditoría guardada en: {filepath}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UTILIDADES CRIPTOGRÁFICAS\n",
    "# ============================================================================\n",
    "\n",
    "class IntegrityValidator:\n",
    "    \"\"\"Validador de integridad de datos\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_hashes(data: str) -> HashMetadata:\n",
    "        \"\"\"\n",
    "        Calcula múltiples hashes para redundancia\n",
    "        \n",
    "        Args:\n",
    "            data: Contenido a hashear\n",
    "            \n",
    "        Returns:\n",
    "            HashMetadata con SHA-256 y MD5\n",
    "        \"\"\"\n",
    "        data_bytes = data.encode('utf-8')\n",
    "        \n",
    "        sha256_hash = hashlib.sha256(data_bytes).hexdigest()\n",
    "        md5_hash = hashlib.md5(data_bytes).hexdigest()\n",
    "        \n",
    "        return HashMetadata(\n",
    "            sha256=sha256_hash,\n",
    "            md5=md5_hash,\n",
    "            timestamp=datetime.now().isoformat()\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def verify_integrity(data: str, expected_sha256: str) -> bool:\n",
    "        \"\"\"\n",
    "        Verifica la integridad de los datos\n",
    "        \n",
    "        Args:\n",
    "            data: Contenido a verificar\n",
    "            expected_sha256: Hash SHA-256 esperado\n",
    "            \n",
    "        Returns:\n",
    "            True si la integridad es válida\n",
    "        \"\"\"\n",
    "        current_hash = hashlib.sha256(data.encode('utf-8')).hexdigest()\n",
    "        return current_hash == expected_sha256\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PROCESADORES DE FUENTES\n",
    "# ============================================================================\n",
    "\n",
    "class DataSourceProcessor:\n",
    "    \"\"\"Procesador genérico de fuentes de datos\"\"\"\n",
    "    \n",
    "    def __init__(self, timeout: int = 30):\n",
    "        self.timeout = timeout\n",
    "        self.session = requests.Session()\n",
    "    \n",
    "    def download_source(self, url: str) -> str:\n",
    "        \"\"\"\n",
    "        Descarga contenido desde una URL\n",
    "        \n",
    "        Args:\n",
    "            url: URL de la fuente\n",
    "            \n",
    "        Returns:\n",
    "            Contenido como texto\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            logger.info(f\"Descarga exitosa: {url}\")\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Error descargando {url}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def process_txt(self, url: str, source_id: str) -> SourceMetadata:\n",
    "        \"\"\"Procesa archivo TXT\"\"\"\n",
    "        logger.info(f\"Procesando TXT: {source_id}\")\n",
    "        \n",
    "        raw_content = self.download_source(url)\n",
    "        hash_meta = IntegrityValidator.compute_hashes(raw_content)\n",
    "        \n",
    "        lines = raw_content.split('\\n')\n",
    "        \n",
    "        metadata = SourceMetadata(\n",
    "            source_id=source_id,\n",
    "            source_name=url.split('/')[-1],\n",
    "            source_type=SourceType.TXT.value,\n",
    "            source_url=url,\n",
    "            size_bytes=len(raw_content.encode('utf-8')),\n",
    "            encoding='utf-8',\n",
    "            processing_timestamp=datetime.now().isoformat(),\n",
    "            hash_metadata=hash_meta,\n",
    "            additional_info={\n",
    "                'total_lines': len(lines),\n",
    "                'non_empty_lines': len([l for l in lines if l.strip()]),\n",
    "                'total_characters': len(raw_content)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def process_csv(self, url: str, source_id: str) -> SourceMetadata:\n",
    "        \"\"\"Procesa archivo CSV con métricas de calidad\"\"\"\n",
    "        logger.info(f\"Procesando CSV: {source_id}\")\n",
    "        \n",
    "        raw_content = self.download_source(url)\n",
    "        hash_meta = IntegrityValidator.compute_hashes(raw_content)\n",
    "        \n",
    "        # Cargar en DataFrame para análisis\n",
    "        df = pd.read_csv(StringIO(raw_content))\n",
    "        \n",
    "        # Métricas de calidad\n",
    "        quality_metrics = DataQualityMetrics(\n",
    "            total_records=len(df),\n",
    "            null_count=int(df.isnull().sum().sum()),\n",
    "            duplicate_count=int(df.duplicated().sum()),\n",
    "            data_types={col: str(dtype) for col, dtype in df.dtypes.items()},\n",
    "            completeness_ratio=float(1 - (df.isnull().sum().sum() / df.size))\n",
    "        )\n",
    "        \n",
    "        metadata = SourceMetadata(\n",
    "            source_id=source_id,\n",
    "            source_name=url.split('/')[-1],\n",
    "            source_type=SourceType.CSV.value,\n",
    "            source_url=url,\n",
    "            size_bytes=len(raw_content.encode('utf-8')),\n",
    "            encoding='utf-8',\n",
    "            processing_timestamp=datetime.now().isoformat(),\n",
    "            hash_metadata=hash_meta,\n",
    "            quality_metrics=quality_metrics,\n",
    "            additional_info={\n",
    "                'rows': len(df),\n",
    "                'columns': len(df.columns),\n",
    "                'column_names': list(df.columns)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def process_html(self, url: str, source_id: str) -> SourceMetadata:\n",
    "        \"\"\"Procesa archivo HTML\"\"\"\n",
    "        logger.info(f\"Procesando HTML: {source_id}\")\n",
    "        \n",
    "        raw_content = self.download_source(url)\n",
    "        hash_meta = IntegrityValidator.compute_hashes(raw_content)\n",
    "        \n",
    "        # Parsear HTML\n",
    "        soup = BeautifulSoup(raw_content, 'lxml')\n",
    "        text_content = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        metadata = SourceMetadata(\n",
    "            source_id=source_id,\n",
    "            source_name=url.split('/')[-1],\n",
    "            source_type=SourceType.HTML.value,\n",
    "            source_url=url,\n",
    "            size_bytes=len(raw_content.encode('utf-8')),\n",
    "            encoding='utf-8',\n",
    "            processing_timestamp=datetime.now().isoformat(),\n",
    "            hash_metadata=hash_meta,\n",
    "            additional_info={\n",
    "                'html_tags_count': len(soup.find_all()),\n",
    "                'text_length': len(text_content),\n",
    "                'has_title': bool(soup.title),\n",
    "                'title': soup.title.string if soup.title else None\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return metadata\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MOTOR DE AUDITORÍA\n",
    "# ============================================================================\n",
    "\n",
    "class AuditEngine:\n",
    "    \"\"\"Motor principal de auditoría y trazabilidad\"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline_version: str = \"1.0.0\"):\n",
    "        self.pipeline_version = pipeline_version\n",
    "        self.processor = DataSourceProcessor()\n",
    "        self.audit_id = self._generate_audit_id()\n",
    "        \n",
    "    def _generate_audit_id(self) -> str:\n",
    "        \"\"\"Genera un ID único para la auditoría\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        random_suffix = hashlib.sha256(str(datetime.now()).encode()).hexdigest()[:8]\n",
    "        return f\"AUDIT_{timestamp}_{random_suffix}\"\n",
    "    \n",
    "    def _get_environment_info(self) -> Dict[str, str]:\n",
    "        \"\"\"Captura información del entorno de ejecución\"\"\"\n",
    "        import sys\n",
    "        import platform\n",
    "        \n",
    "        return {\n",
    "            'python_version': sys.version,\n",
    "            'platform': platform.platform(),\n",
    "            'machine': platform.machine(),\n",
    "            'processor': platform.processor(),\n",
    "            'execution_user': os.getenv('USER', 'unknown')\n",
    "        }\n",
    "    \n",
    "    def run_audit(self, sources: Dict[str, str]) -> AuditRecord:\n",
    "        \"\"\"\n",
    "        Ejecuta auditoría completa de todas las fuentes\n",
    "        \n",
    "        Args:\n",
    "            sources: Diccionario {source_id: url}\n",
    "            \n",
    "        Returns:\n",
    "            AuditRecord con resultados completos\n",
    "        \"\"\"\n",
    "        logger.info(f\"Iniciando auditoría: {self.audit_id}\")\n",
    "        \n",
    "        processed_sources = []\n",
    "        successful = 0\n",
    "        failed = 0\n",
    "        warnings = []\n",
    "        errors = []\n",
    "        \n",
    "        for source_id, url in sources.items():\n",
    "            try:\n",
    "                # Determinar tipo de procesador\n",
    "                extension = url.split('.')[-1].lower()\n",
    "                \n",
    "                if extension == 'txt':\n",
    "                    metadata = self.processor.process_txt(url, source_id)\n",
    "                elif extension == 'csv':\n",
    "                    metadata = self.processor.process_csv(url, source_id)\n",
    "                elif extension in ['html', 'htm']:\n",
    "                    metadata = self.processor.process_html(url, source_id)\n",
    "                else:\n",
    "                    warnings.append(f\"Tipo no soportado para {source_id}: {extension}\")\n",
    "                    continue\n",
    "                \n",
    "                processed_sources.append(metadata)\n",
    "                successful += 1\n",
    "                \n",
    "                # Verificar calidad de datos si aplica\n",
    "                if hasattr(metadata, 'quality_metrics') and metadata.quality_metrics:\n",
    "                    if metadata.quality_metrics.completeness_ratio < 0.95:\n",
    "                        warnings.append(\n",
    "                            f\"{source_id}: Completitud baja ({metadata.quality_metrics.completeness_ratio:.2%})\"\n",
    "                        )\n",
    "                \n",
    "            except Exception as e:\n",
    "                failed += 1\n",
    "                error_msg = f\"Error procesando {source_id}: {str(e)}\"\n",
    "                errors.append(error_msg)\n",
    "                logger.error(error_msg)\n",
    "        \n",
    "        # Determinar estado general\n",
    "        if failed == 0:\n",
    "            status = AuditStatus.SUCCESS.value\n",
    "        elif successful > 0:\n",
    "            status = AuditStatus.WARNING.value\n",
    "        else:\n",
    "            status = AuditStatus.FAILURE.value\n",
    "        \n",
    "        # Crear registro de auditoría\n",
    "        audit_record = AuditRecord(\n",
    "            audit_id=self.audit_id,\n",
    "            pipeline_version=self.pipeline_version,\n",
    "            execution_timestamp=datetime.now().isoformat(),\n",
    "            sources_processed=processed_sources,\n",
    "            audit_status=status,\n",
    "            total_sources=len(sources),\n",
    "            successful_sources=successful,\n",
    "            failed_sources=failed,\n",
    "            warnings=warnings,\n",
    "            errors=errors,\n",
    "            environment_info=self._get_environment_info()\n",
    "        )\n",
    "        \n",
    "        # Guardar auditoría\n",
    "        audit_filepath = os.path.join(\n",
    "            AUDIT_DIR, \n",
    "            f\"{self.audit_id}.json\"\n",
    "        )\n",
    "        audit_record.save_to_json(audit_filepath)\n",
    "        \n",
    "        # Generar reporte en consola\n",
    "        self._print_audit_summary(audit_record)\n",
    "        \n",
    "        return audit_record\n",
    "    \n",
    "    def _print_audit_summary(self, audit: AuditRecord) -> None:\n",
    "        \"\"\"Imprime resumen de auditoría en consola\"\"\"\n",
    "        print(f\"RESUMEN DE AUDITORÍA: {audit.audit_id}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Estado: {audit.audit_status.upper()}\")\n",
    "        print(f\"Fuentes totales: {audit.total_sources}\")\n",
    "        print(f\"Exitosas: {audit.successful_sources}\")\n",
    "        print(f\"Fallidas: {audit.failed_sources}\")\n",
    "        print(f\"Advertencias: {len(audit.warnings)}\")\n",
    "        print(f\"Errores: {len(audit.errors)}\")\n",
    "        print(\"\\nFuentes procesadas:\")\n",
    "        for source in audit.sources_processed:\n",
    "            print(f\"  ✓ {source.source_id} ({source.source_type})\")\n",
    "            print(f\"    SHA-256: {source.hash_metadata.sha256}\")\n",
    "            print(f\"    Tamaño: {source.size_bytes:,} bytes\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Programa de ejecución final:\n",
    "def main():\n",
    "    \"\"\"Función principal de ejecución\"\"\"\n",
    "    print(\"Iniciando Sistema de Auditoría y Trazabilidad\")\n",
    "    print(\"Universidad Politécnica Metropolitana de Hidalgo\\n\")\n",
    "    \n",
    "    # Crear motor de auditoría\n",
    "    engine = AuditEngine(pipeline_version=\"1.0.0\")\n",
    "    \n",
    "    # Ejecutar auditoría completa\n",
    "    audit_result = engine.run_audit(DATA_SOURCES)\n",
    "    \n",
    "    # Generar DataFrame con resumen\n",
    "    summary_data = []\n",
    "    for source in audit_result.sources_processed:\n",
    "        summary_data.append({\n",
    "            'Source ID': source.source_id,\n",
    "            'Type': source.source_type,\n",
    "            'Size (bytes)': source.size_bytes,\n",
    "            'SHA-256': source.hash_metadata.sha256[:16] + '...',\n",
    "            'Timestamp': source.processing_timestamp\n",
    "        })\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    print(\"\\nResumen de Fuentes Procesadas:\")\n",
    "    print(df_summary.to_string(index=False))\n",
    "    \n",
    "    return audit_result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audit_result = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
